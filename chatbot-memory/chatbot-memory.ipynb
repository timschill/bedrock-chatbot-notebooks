{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory types in LangChain\n",
    "This notebook showcase different memory types in LangChain and how we can use them in a chatbot.  \n",
    "We work with Amazon Bedrock and the FM Claude V2 from Anthropic in this example.  \n",
    "\n",
    "Start by importing the libraries for LangChain and boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Bedrock client and model params\n",
    "Next we will create out boto3.client and Bedrock objects together with some model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_runtime = boto3.client(\n",
    "        service_name=\"bedrock-runtime\",\n",
    "        region_name=\"us-east-1\",\n",
    ")\n",
    "\n",
    "model_kwargs = {\n",
    "    \"max_tokens_to_sample\": 300,\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_p\": 0.5\n",
    "}\n",
    "\n",
    "chat = Bedrock(\n",
    "    credentials_profile_name=\"bedrock\",\n",
    "    model_id=\"anthropic.claude-v2\",\n",
    "    model_kwargs=model_kwargs,\n",
    "    client=bedrock_runtime,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt Template\n",
    "We will then create our prompt template like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Assistant: The following is a friendly conversation between a knowledgeable helpful assistant and a customer.\n",
    "The assistant is talkative and provides lots of specific details from it's context.\n",
    "\n",
    "Conversation history:\n",
    "{history}\n",
    "\n",
    "Current conversation:\n",
    "Human: {input}\n",
    "Assistant:\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "        input_variables=[\"history\", \"input\"], template=prompt_template\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory types\n",
    "LangChain offers a wide range of different memory options, the most common, \"ConversationBufferMemory\" will store all previous inputs and outputs in a list.  \n",
    "We configure the memory like this, please note that when we work with Claude V2 we have to change our ai_prefix to \"Assistant\" and the human_prefix to \"Human\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(ai_prefix=\"Assistant\", human_prefix=\"Human\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conversation chain\n",
    "Then we configure our ConversationChain to use our prompt, llm and memory like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation = ConversationChain(\n",
    "    prompt=PROMPT,\n",
    "    llm=chat,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Query\n",
    "We can now test to query our chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Can cats fly?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ConversationBufferWindowMemory\n",
    "What if we dont want to return the hole conversation history, but only the K most recent interactions?  \n",
    "ConversationBufferWindowMemory comes to the rescue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "memory = None   # Reset memory\n",
    "conversation = None # Reset conversation\n",
    "memory = ConversationBufferWindowMemory(k=1, ai_prefix=\"Assistant\", human_prefix=\"Human:\")\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    prompt=PROMPT,\n",
    "    llm=chat,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"Hi there!\")\n",
    "conversation.predict(input=\"Can cats fly?\")\n",
    "conversation.predict(input=\"Cool, can you tell me more about cats?\")\n",
    "conversation.predict(input=\"Have we greeted each other yet?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keep the token usage down, ConversationSummaryMemory\n",
    "As we can see only the last previous message is included in the history.  \n",
    "What if we want to give it a longer memory, but still want to keep our token usage down?  \n",
    "Well thats when ConversationSummaryMemory comes in handy.  \n",
    "It will summarize the previous conversation and add it to the prompt.\n",
    "\n",
    "The summarization is handled by the LLM it self so we have to add it as an attribute to ConversationSummaryMemory object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "memory = None   # Reset memory\n",
    "conversation = None # Reset conversation\n",
    "memory = ConversationSummaryMemory(llm=chat, human_prefix=\"Human\", ai_prefix=\"Assistant\")\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    prompt=PROMPT,\n",
    "    llm=chat,\n",
    "    verbose=False,\n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"Hi there!\")\n",
    "conversation.predict(input=\"Can cats fly?\")\n",
    "conversation.predict(input=\"Cool, can you tell me more about cats?\")\n",
    "conversation.predict(input=\"Have we greeted each other yet?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please note that we set the verbose attribute to false this time. That is because LangChain utilize a template where the AI prefix has been hardcoded to \"AI\", and this throws a lot of warnings in the console.\n",
    "The end result is fine though.\n",
    "\n",
    "#### ConversationSummaryBufferMemory\n",
    "Here we will mix the summary memory with the buffer memory, with the attribute \"max_token_limit\" we specify how many tokens we want to keep in the buffer then we summarize the older conversations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "\n",
    "memory = None   # Reset memory\n",
    "conversation = None # Reset conversation\n",
    "memory = ConversationSummaryBufferMemory(llm=chat, max_token_limit=25, human_prefix=\"Human\", ai_prefix=\"Assistant\")\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    prompt=PROMPT,\n",
    "    llm=chat,\n",
    "    verbose=False,\n",
    "    memory=memory,\n",
    ")\n",
    "\n",
    "conversation.predict(input=\"Hi there!\")\n",
    "conversation.predict(input=\"Can cats fly?\")\n",
    "conversation.predict(input=\"Cool, can you tell me more about cats?\")\n",
    "conversation.predict(input=\"Have we greeted each other yet?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bedrock-chatbot-notebooks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
